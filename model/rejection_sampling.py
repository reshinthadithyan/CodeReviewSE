from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
import torch
import logging
import argparse
from model.cross_encoder_utils.model import CrossEncoder

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


device_dict = {"ce": "cuda:0", "codegen": "cuda:1"}


class RejectionSampler:
    def __init__(
        self,
        generation_model_name_or_path: str,
        generation_tokenizer_name_or_path: str,
        ce_tokenizer_name_or_path: str,
        ce_name_or_path: str,
        device_dict: dict,
    ):
        self.ce_tokenizer = AutoTokenizer.from_pretrained(ce_tokenizer_name_or_path)
        self.ce_model = CrossEncoder(ce_name_or_path).to(device_dict["ce"])
        logger.info("Sucessfully loaded ce_model to device...")
        self.generation_tokenizer = AutoTokenizer.from_pretrained(
            generation_tokenizer_name_or_path
        )
        self.generation_model = AutoModelForCausalLM.from_pretrained(
            generation_model_name_or_path
        ).to(device_dict["codegen"])
        logger.info("Sucessfully Loaded generation_model to device...")
        self.ce_model.eval()
        self.generation_model.eval()

    def generate_datapoints(self, examples: list[str]):
        """
        args:
            examples (list[str]): list of examples to be generated and sampled.
        """
        tokenized = self.generation_tokenizer(
            examples, return_tensors="pt", padding=True, truncation=True
        )
        generated = self.generation_model.generate(tokenized.input_ids)
        generated_text = self.generation_tokenizer.batch_decode(
            generated, skip_special_tokens=True
        )
        return generated_text

    def rank_one_sample_using_ce(self, examples: list[str]) -> list[str]:
        """
        args:

            examples (list[str]): list of critques generated by the model.
        """
        tokenized = self.ce_tokenizer(
            examples, return_tensors="pt", padding=True, truncation=True
        )
        with torch.no_grad():
            _, logits = self.ce_model(
                **tokenized
            )  # input_ids, attention_mask,token_type_ids
        sorted_logits_indices = torch.argsort(logits, dim=0, descending=True)
        return [examples[i] for i in sorted_logits_indices[0].item()]

    def __call__(self, examples: list[str]):
        """
        args:
            examples (list[str]): list of examples to be generated and sampled.
        """
        generated_text = self.generate_datapoints(examples)
        return self.rank_one_sample_using_ce(generated_text)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument("--code_gen_model_name_or_path", type=str, default="Salesforce/codegen-350M-mono")
    parser.add_argument("--generation_tokenizer_name_or_path", type=str, default="Salesforce/codegen-350M-mono")
    parser.add_argument("--ce_name_or_path", type=str, default="CarperAI/carptriever-1")
    parser.add_argument("--ce_tokenizer_name_or_path", type=str, default="CarperAI/carptriever-1")

    args = parser.parse_args()

    rejection_sampler = RejectionSampler(
        args.genertion_model_name_or_path,
        args.generation_tokenizer_name_or_path,
        args.ce_tokenizer_name_or_path,
        args.ce_name_or_path,
        device_dict,
    )

